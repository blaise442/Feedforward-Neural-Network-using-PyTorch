{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7351cbe-ff79-4fac-93ac-81f4fbed2926",
   "metadata": {},
   "source": [
    "## Name : Blaise Wangmeni \n",
    "## Student ID : 2041120\n",
    "## course: COSC6328\n",
    "## course Name :Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873d5d7-723d-476f-afa2-0b258218ae9b",
   "metadata": {},
   "source": [
    "### Building a Feedforward Neural Network with PyTorch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d37e5-1e2d-4ae4-8d3b-8829160e339c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 1: Load Dataset\n",
    "#### The first step is to load the  the famous Iris flower dataset, which you can download from the UCI Machine Learning Repository at this link: https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e653c4-046e-4a3e-9f3a-08d8cf0e9941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris = pd.read_csv(url, header=None)\n",
    "\n",
    "# Convert the dataset to PyTorch tensors\n",
    "X = torch.tensor(iris.iloc[:, :-1].values).float()\n",
    "y = torch.tensor(pd.Categorical(iris.iloc[:, -1]).codes).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f2cad81-882d-4ae2-912b-fe2915087680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5c2bfc1-2711-450e-a6a3-f75f8327625f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       150 non-null    float64\n",
      " 1   1       150 non-null    float64\n",
      " 2   2       150 non-null    float64\n",
      " 3   3       150 non-null    float64\n",
      " 4   4       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "320837ab-0ef1-4ad7-841e-0a5c7ee8b769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaff14b-8603-4437-aa45-6877c93d9b8d",
   "metadata": {},
   "source": [
    "### Step 2: Make Dataset Iterable\n",
    "#### We will now create a PyTorch Dataset and DataLoader objects to make the dataset iterable during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03422848-840b-402b-8c41-6b835b0c92ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define the dataset\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Define the data loader\n",
    "batch_size = 10\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b07f851-f458-49a7-bcaa-b44fd61099e9",
   "metadata": {},
   "source": [
    "### Step 3: Create Model Class\n",
    "#### We will now define our logistic regression model as a class that inherits from torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "047ac10f-d0ac-4e30-ac57-3327e2c1cf32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2b093-682c-41a9-88ca-58d573f1138c",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate Model Class\n",
    "#### We will now instantiate our model and define the input and output dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85810178-89f2-41bc-a42d-89cea3bc4055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the input and output dimensions\n",
    "input_dim = 4\n",
    "output_dim = 3\n",
    "\n",
    "# Instantiate the model\n",
    "model = LogisticRegression(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010524e2-50d0-4a57-8168-519acd42aedf",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate Loss Class\n",
    "#### We will use the cross-entropy loss function, which is suitable for multi-class classification problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1959dc8b-2efd-4b5b-b0d0-0ea19b09cc80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe6b8e-81e7-49d7-8e3e-0fa699a787ab",
   "metadata": {},
   "source": [
    "### Step 6: Instantiate Optimizer Class\n",
    "\n",
    "#### We will be using the stochastic gradient descent (SGD) optimizer to train our model. We can instantiate the optimizer class using the torch.optim.SGD() function in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "434c68e5-8a7d-4094-9d52-b478533706ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the learning rate and the optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec3d44-a665-4c6a-9daf-eadfac35f387",
   "metadata": {},
   "source": [
    "### Step 7: Train Model\n",
    "\n",
    "#### We will now train the model for a certain number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf16bf82-f4a2-4e09-853f-1304d88d0d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/15], Loss: 1.3509\n",
      "Epoch [2/50], Step [10/15], Loss: 1.2404\n",
      "Epoch [3/50], Step [10/15], Loss: 1.0304\n",
      "Epoch [4/50], Step [10/15], Loss: 0.9072\n",
      "Epoch [5/50], Step [10/15], Loss: 0.8501\n",
      "Epoch [6/50], Step [10/15], Loss: 0.8494\n",
      "Epoch [7/50], Step [10/15], Loss: 0.7850\n",
      "Epoch [8/50], Step [10/15], Loss: 0.7084\n",
      "Epoch [9/50], Step [10/15], Loss: 0.6500\n",
      "Epoch [10/50], Step [10/15], Loss: 0.6763\n",
      "Epoch [11/50], Step [10/15], Loss: 0.7188\n",
      "Epoch [12/50], Step [10/15], Loss: 0.6306\n",
      "Epoch [13/50], Step [10/15], Loss: 0.5033\n",
      "Epoch [14/50], Step [10/15], Loss: 0.6235\n",
      "Epoch [15/50], Step [10/15], Loss: 0.5538\n",
      "Epoch [16/50], Step [10/15], Loss: 0.5548\n",
      "Epoch [17/50], Step [10/15], Loss: 0.6278\n",
      "Epoch [18/50], Step [10/15], Loss: 0.5626\n",
      "Epoch [19/50], Step [10/15], Loss: 0.6738\n",
      "Epoch [20/50], Step [10/15], Loss: 0.6624\n",
      "Epoch [21/50], Step [10/15], Loss: 0.3644\n",
      "Epoch [22/50], Step [10/15], Loss: 0.6540\n",
      "Epoch [23/50], Step [10/15], Loss: 0.3536\n",
      "Epoch [24/50], Step [10/15], Loss: 0.4277\n",
      "Epoch [25/50], Step [10/15], Loss: 0.3616\n",
      "Epoch [26/50], Step [10/15], Loss: 0.5315\n",
      "Epoch [27/50], Step [10/15], Loss: 0.4543\n",
      "Epoch [28/50], Step [10/15], Loss: 0.4674\n",
      "Epoch [29/50], Step [10/15], Loss: 0.3992\n",
      "Epoch [30/50], Step [10/15], Loss: 0.4740\n",
      "Epoch [31/50], Step [10/15], Loss: 0.5980\n",
      "Epoch [32/50], Step [10/15], Loss: 0.4849\n",
      "Epoch [33/50], Step [10/15], Loss: 0.3874\n",
      "Epoch [34/50], Step [10/15], Loss: 0.4100\n",
      "Epoch [35/50], Step [10/15], Loss: 0.4335\n",
      "Epoch [36/50], Step [10/15], Loss: 0.3456\n",
      "Epoch [37/50], Step [10/15], Loss: 0.3745\n",
      "Epoch [38/50], Step [10/15], Loss: 0.3820\n",
      "Epoch [39/50], Step [10/15], Loss: 0.4867\n",
      "Epoch [40/50], Step [10/15], Loss: 0.3353\n",
      "Epoch [41/50], Step [10/15], Loss: 0.4192\n",
      "Epoch [42/50], Step [10/15], Loss: 0.4642\n",
      "Epoch [43/50], Step [10/15], Loss: 0.4067\n",
      "Epoch [44/50], Step [10/15], Loss: 0.4440\n",
      "Epoch [45/50], Step [10/15], Loss: 0.4234\n",
      "Epoch [46/50], Step [10/15], Loss: 0.4120\n",
      "Epoch [47/50], Step [10/15], Loss: 0.3569\n",
      "Epoch [48/50], Step [10/15], Loss: 0.4137\n",
      "Epoch [49/50], Step [10/15], Loss: 0.2941\n",
      "Epoch [50/50], Step [10/15], Loss: 0.3398\n",
      "Accuracy of the model on the test dataset: 96.0 %\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss every 10 iterations\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(data_loader), loss.item()))\n",
    "\n",
    "# Test the model with a test dataset\n",
    "test_dataset = TensorDataset(X, y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the model on the test dataset: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6174b6-10a7-40f3-ba98-f685a75eeef3",
   "metadata": {},
   "source": [
    "### This code loops over the test dataset, computes the outputs of the model, and compares them to the actual labels to compute the accuracy of the model. The result is printed out as a percentage in this case 96.0 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa412dde-1687-48e0-a73d-67ee741d8bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
